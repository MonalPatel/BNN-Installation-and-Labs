{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cifar-10 testset classification on Pynq\n",
    "\n",
    "This notebook covers how to use low quantized neural networks on Pynq. \n",
    "It shows an example how CIFAR-10 testset can be inferred utilizing different precision neural networks inspired at VGG-16, featuring 6 convolutional layers, 3 max pool layers and 3 fully connected layers. There are 3 different precision available:\n",
    "\n",
    "- CNVW1A1 using 1 bit weights and 1 bit activation,\n",
    "- CNVW1A2 using 1 bit weights and 2 bit activation and\n",
    "- CNVW2A2 using 2 bit weights and 2 bit activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%microblaze/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import bnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Cifar-10 testset\n",
    "\n",
    "This notebook required the testset from https://www.cs.toronto.edu/~kriz/cifar.html which contains 10000 images that can be processed by CNV network directly without preprocessing.\n",
    "\n",
    "You can download the cifar-10 set from given url via wget and unzip it to a folder on Pynq as shown below.\n",
    "This may take a while as the training set is included in the archive as well.\n",
    "After that we need to read the labels from the binary file to be able to compare the results later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get\n",
    "!wget https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\n",
    "#unzip\n",
    "!tar -xf cifar-10-binary.tar.gz\n",
    "\n",
    "labels = []\n",
    "with open(\"/home/xilinx/jupyter_notebooks/bnn/cifar-10-batches-bin/test_batch.bin\", \"rb\") as file:\n",
    "    #for 10000 pictures\n",
    "    for i in range(10000):\n",
    "        #read first byte -> label\n",
    "        labels.append(int.from_bytes(file.read(1), byteorder=\"big\"))\n",
    "        #read image (3072 bytes) and do nothing with it\n",
    "        file.read(3072)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Start inference\n",
    "\n",
    "The inference can be performed with different precision for weights and activation. Creating a specific Classifier will automatically download the correct bitstream onto PL and load the weights and thresholds trained on the specific dataset. \n",
    "\n",
    "Thus that images are already Cifar-10 preformatted no preprocessing is required. Therefor the functions `classify_cifar` or `classify_cifars` can be used. When classifying non Cifar-10 formatted pictures refer to `classify_image` or `classify_images`  (see Notebook CNV-QNN_Cifar10).\n",
    "\n",
    "### Case 1: \n",
    "#### W1A1 - 1 bit weight and 1 activation\n",
    "\n",
    "Instantiate the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_classifier = bnn.CnvClassifier(bnn.NETWORK_CNVW1A1,'cifar10',bnn.RUNTIME_HW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And start the inference on Cifar-10 preformatted multiple images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference took 3278691.10 microseconds, 327.87 usec per image\n",
      "Classification rate: 3050.00 images per second\n"
     ]
    }
   ],
   "source": [
    "result_W1A1 = hw_classifier.classify_cifars(\"/home/xilinx/jupyter_notebooks/bnn/cifar-10-batches-bin/test_batch.bin\")\n",
    "time_W1A1 = hw_classifier.usecPerImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2:\n",
    "#### W1A2 - 1 bit weight and 2 activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_classifier = bnn.CnvClassifier(bnn.NETWORK_CNVW1A2,'cifar10',bnn.RUNTIME_HW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference took 3278505.86 microseconds, 327.85 usec per image\n",
      "Classification rate: 3050.17 images per second\n"
     ]
    }
   ],
   "source": [
    "result_W1A2 = hw_classifier.classify_cifars(\"/home/xilinx/jupyter_notebooks/bnn/cifar-10-batches-bin/test_batch.bin\")\n",
    "time_W1A2 = hw_classifier.usecPerImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3:\n",
    "#### W2A2 - 2 bit weight and 2 activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_classifier = bnn.CnvClassifier(bnn.NETWORK_CNVW2A2,'cifar10',bnn.RUNTIME_HW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference took 11626531.98 microseconds, 1162.65 usec per image\n",
      "Classification rate: 860.10 images per second\n"
     ]
    }
   ],
   "source": [
    "result_W2A2 = hw_classifier.classify_cifars(\"/home/xilinx/jupyter_notebooks/bnn/cifar-10-batches-bin/test_batch.bin\")\n",
    "time_W2A2 = hw_classifier.usecPerImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference time\n",
    "\n",
    "Results can be visualized using `matplotlib`. Here the comparison of hardware execution time is plotted in microseconds per Image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ac50d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "height = [time_W1A1, time_W1A2, time_W2A2]\n",
    "bars   = ('W1A1', 'W1A2', 'W2A2')\n",
    "\n",
    "y_pos=range(3)\n",
    "plt.bar(y_pos, height, 0.5)\n",
    "plt.xticks(y_pos, bars)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "The accuracy on the testset can be calculated by comparing the inferred labels against the one read at the beginning:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy W1A1:  78.52 %\n",
      "Accuracy W1A2:  81.89 %\n",
      "Accuracy W2A2:  83.79 %\n"
     ]
    }
   ],
   "source": [
    "#compare against labels\n",
    "countRight = 0\n",
    "for idx in range(len(labels)):\n",
    "    if labels[idx] == result_W1A1[idx]:\n",
    "        countRight += 1\n",
    "accuracyW1A1 = countRight*100/len(labels)\n",
    "\n",
    "countRight = 0\n",
    "for idx in range(len(labels)):\n",
    "    if labels[idx] == result_W1A2[idx]:\n",
    "        countRight += 1\n",
    "accuracyW1A2 = countRight*100/len(labels)\n",
    "\n",
    "countRight = 0\n",
    "for idx in range(len(labels)):\n",
    "    if labels[idx] == result_W2A2[idx]:\n",
    "        countRight += 1\n",
    "accuracyW2A2 = countRight*100/len(labels)\n",
    "\n",
    "print(\"Accuracy W1A1: \",accuracyW1A1,\"%\")\n",
    "print(\"Accuracy W1A2: \",accuracyW1A2,\"%\")\n",
    "print(\"Accuracy W2A2: \",accuracyW2A2,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reset the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynq import Xlnk\n",
    "\n",
    "xlnk = Xlnk()\n",
    "xlnk.xlnk_reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
